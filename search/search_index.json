{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Avance Proyecto de programaci\u00f3n","text":"<p>Integrantes del grupo</p> <ol> <li>Denzel Dar\u00edo Guzm\u00e1n Carranza</li> <li>Josu\u00e9 Garc\u00eda Bland\u00f3n</li> </ol> <p>En esta documentaci\u00f3n se observar\u00e1 la exploraci\u00f3n de los datos realizada  tras su recopilaci\u00f3n.</p> <p>Dentro de la base de datos llamada como proyecto.db se guardan los datos recopilados y, aprovechando las herramientas de programaci\u00f3n de python se definir\u00e1n los aspectos escenciales de la distribuci\u00f3n de probabilidad que conforman las variables 1 y 2.</p>"},{"location":"#definiciones-importantes","title":"Definiciones importantes","text":"<p>Funci\u00f3n de distribuci\u00f3n de probabilidad</p> <p>La funci\u00f3n de densidad de probabilidad se define como la probabilidad seg\u00fan la cual una variable aleatoria puede adquirir un valor determinado </p> <p>Histograma</p> <p>Un histograma se define brevemente como una forma de representaci\u00f3n gr\u00e1fica de la recurrencia con la que sucede un determinado evento. Sirve para simbolizar la distribuci\u00f3n de un conjunto de datos.</p>"},{"location":"#tipos-de-modelos-de-distribucion-de-probabilidad","title":"Tipos de modelos de distribuci\u00f3n de probabilidad","text":"<ol> <li>Distribuci\u00f3n normal</li> <li>Exponencial</li> <li>Rayleigh</li> <li>Uniforme</li> <li>Bernoulli</li> <li>Boltzmann</li> </ol> <p>Y muchos otros modelos de distribuci\u00f3n, tantos que no ser\u00eda recomendable agregarlos todos a la lista.</p>"},{"location":"#pasos-realizados-para-la-recopilacion-de-datos","title":"Pasos realizados para la recopilaci\u00f3n de datos","text":"<ol> <li>Important\u00edsimo siempre activar el ambiente de python antes de realizar cualquier acci\u00f3n. </li> <li>En tres terminales separadas activar redis-server, celery tasks y celery beat con el fin de iniciar la toma de datos.</li> <li>Eliminar la variable 3 del c\u00f3digo del repositorio ya que esta no est\u00e1 contemplada en el funcionamiento regular.</li> <li>Una vez el c\u00f3digo est\u00e9 trabajando consistentemente, dejarlo activo en segundo plano durante al menos 12 horas seguidas.</li> <li>Asegurarse de que los datos se est\u00e9n actualizando con cada ciclo y que el n\u00famero de datos aumente de forma gradual a medida que pasan las horas.</li> </ol>"},{"location":"#repositorio-remoto-y-github","title":"Repositorio remoto y github","text":"<p>Para la realizaci\u00f3n de este proyecto es sumamente importante mantener un contacto ordenado y coordinado con los miembros del grupo. Es por ello que la plataforma github es sumamente \u00fatil para mejorar la coordinaci\u00f3n y agilizar el trabajo.</p>"},{"location":"analisis_conclusiones/","title":"An\u00e1lisis y Conclusiones","text":""},{"location":"analisis_conclusiones/#1-determinacion-de-la-funcion-de-densidad-de-probabilidad-fdp","title":"1. Determinaci\u00f3n de la Funci\u00f3n de Densidad de Probabilidad (FDP)","text":""},{"location":"analisis_conclusiones/#analisis","title":"An\u00e1lisis","text":"<p>A partir de los procedimientos realizados y a paritir de los programas creados, primero se logr\u00f3 determinar que el modelo de distribuci\u00f3n de mejor ajuste para los datos recopilados es de tipo exponencial. Se obtuvieron los par\u00e1metros de loc y scale para cada instante de tiempo en el que se jalaron los datos de la API. </p> <p>Como bien se determin\u00f3, la media cambiaba respecto al tiempo, espec\u00edficamente de las condiciones de luz del d\u00eda y la noche. Al observar el gr\u00e1fico de promedio contra tiempo, se determin\u00f3 que esta se manten\u00eda relativamente constante alrededor de 1 cuando es de noche y formaba una curva cuadr\u00e1tica cuando era de d\u00eda. Al obtener la curva de mejor ajuste de esta cuadr\u00e1tica se pudo obtener una ecuaci\u00f3n de la media en funci\u00f3n del tiempo, de tal manera que, al realizar el inverso de la media temporal se encontr\u00f3 el par\u00e1metro lambda (\\(\\lambda(t)\\)) necesario para la  PDF de un modelo de distribuci\u00f3n exponencial.</p> <p>Se utiliz\u00f3 la media para obtener la funci\u00f3n de densidad de probabilidad en lugar de los par\u00e1metros loc y scale debido a que, el par\u00e1metro loc t\u00edpicamente es cero, por lo que este se despreci\u00f3 y, seg\u00fan la documentaci\u00f3n el par\u00e1metro scale es igual a la media para el tipo de distribuci\u00f3n que se est\u00e1 trabajando, es por ello que, por simplicidad se decidi\u00f3 aprovechar la media.</p>"},{"location":"analisis_conclusiones/#2-determinacion-de-la-estacionaridad-en-sentido-amplio-wss-y-ergodicidad","title":"2. Determinaci\u00f3n de la Estacionaridad en Sentido Amplio (WSS) y Ergodicidad","text":""},{"location":"analisis_conclusiones/#analisis_1","title":"An\u00e1lisis","text":"<p>Estacionaridad:</p> <ul> <li>Para \\( sunlight = 0 \\), el proceso cumple con las condiciones de WSS, ya que la media y la autocorrelaci\u00f3n son constantes o dependen \u00fanicamente del diferencial de tiempo \\(\\tau\\) que, en este caso fue de aproximadamente 42 minutos.</li> <li>Para \\( sunlight = 1 \\), el proceso no es estacionario debido a la variabilidad temporal de la media y la potencia, es decir que el proceso var\u00eda con el tiempo, por lo que es muy sencillo indicar que el proceso no es estacionario en su totalidad.</li> </ul> <p>Ergodicidad:</p> <ul> <li>En \\( sunlight = 0 \\), la media temporal (\\( \\overline{x} \\)) es igual a la media estad\u00edstica (\\( \\overline{X} \\)), confirmando la ergodicidad en condiciones nocturnas cuando el proceso es confirmado como estacionario en sentido amplio.</li> <li>Para \\( sunlight = 1 \\), la no estacionaridad limita la evaluaci\u00f3n erg\u00f3dica directa pero se puede confirmar que, para las condiciones nocturnas el proceso no es erg\u00f3dico.</li> </ul>"},{"location":"analisis_conclusiones/#implicacion","title":"Implicaci\u00f3n","text":"<p>El comportamiento estacionario y erg\u00f3dico en \\( sunlight = 0 \\) simplifica el an\u00e1lisis y dise\u00f1o de sistemas. Sin embargo, las condiciones diurnas requieren m\u00e9todos m\u00e1s avanzados para predecir y modelar el proceso.</p>"},{"location":"analisis_conclusiones/#3-determinacion-de-potencia-promedio","title":"3. Determinaci\u00f3n de Potencia Promedio","text":""},{"location":"analisis_conclusiones/#analisis_2","title":"An\u00e1lisis","text":"<p>Para \\( sunlight = 0 \\):</p> <ul> <li>La potencia promedio es constante (\\( P = 2.00 \\)), lo que refleja la estabilidad del proceso en condiciones nocturnas. Dado que el proceso era estacionario en sentido amplio, el cuadrado de los datos durante la noche tambi\u00e9n es estacionario en sentido amplio, es por ello que la potencia promedio se obtuvo simplemente como el valor esperado del valor cuadr\u00e1tico medio o la media del cuadrado \\(E[\\overline{X^{2}}]\\).</li> </ul> <p>Para \\( sunlight = 1 \\):</p> <ul> <li>La potencia promedio var\u00eda cuadr\u00e1ticamente con el tiempo. El ajuste cuadr\u00e1tico permite calcular un valor promedio temporal (\\( P = 11.1619 \\)), reflejando mayor intensidad durante el d\u00eda. Dado que el proceso durante el d\u00eda es din\u00e1mico o que var\u00eda con el tiempo, fue necesario aplicar la media del valor cuadr\u00e1tico medio a trav\u00e9s de integrales y la ecuaci\u00f3n de mejor ajuste de los datos.</li> </ul>"},{"location":"analisis_conclusiones/#implicacion_1","title":"Implicaci\u00f3n","text":"<p>La potencia promedio est\u00e1tica valida la estabilidad del proceso durante la noche, mientras que el ajuste cuadr\u00e1tico captura de manera efectiva la din\u00e1mica diurna.</p>"},{"location":"analisis_conclusiones/#conclusiones-generales","title":"Conclusiones Generales","text":"<ol> <li> <p>Modelo Estad\u00edstico    La distribuci\u00f3n mejor ajustada a los datos obtenidos de la API fue un factor influyente en la toma de decisiones para el resto de pasos del proyecto. Debido a la simplicidad del modelo exponencial, no fue necesario comprobar el comportamiento temporal de los par\u00e1metros brindados por <code>fit</code> ya que habr\u00eda sido un doble trabajo. </p> </li> <li> <p>Estabilidad y Variabilidad    El proceso es estacionario y erg\u00f3dico durante la noche (\\( sunlight = 0 \\)), mientras que en el d\u00eda (\\( sunlight = 1 \\)), su complejidad requiere enfoques avanzados para an\u00e1lisis y predicci\u00f3n. En telecomunicaciones y el estudio de procesos complejos y variantes con el tiempo, factores como el clima, las condiciones de luz entre otros pueden dictar el comportamiento de un proceso, es por ello que, para poder intentar predecir lo que suceder\u00e1 con un determinado proceso, transmisi\u00f3n, u estad\u00edstica se deben tomar en cuenta todas estas variables tanto din\u00e1micas como est\u00e1ticas.</p> </li> <li> <p>Potencia Promedio    La estabilidad nocturna de la potencia promedio simplifica los c\u00e1lculos y an\u00e1lisis. El ajuste cuadr\u00e1tico para el d\u00eda ofrece una herramienta \u00fatil para modelar variaciones temporales.</p> </li> <li> <p>Recomendaciones    a) Ampliar el an\u00e1lisis a m\u00e1s variables o condiciones para validar y refinar los modelos actuales.    b) Incorporar t\u00e9cnicas avanzadas como an\u00e1lisis espectral o wavelets para abordar la no estacionaridad en \\( sunlight = 1 \\).</p> </li> </ol>"},{"location":"densidad/","title":"Funci\u00f3n de densidad de probabilidad","text":""},{"location":"densidad/#funcion-de-densidad-de-probabilidad","title":"Funci\u00f3n de densidad de probabilidad","text":"<p>Para determinar la funci\u00f3n de densidad de probabilidad del proceso aleatorio, es importante primero determinar el modelo de distribuci\u00f3n del proceso. Esto se logra a trav\u00e9s de la funci\u00f3n fitter aplicado a los datos recopilados mediante el programa <code>distribucion.py</code> el cual se muestra a continuaci\u00f3n:</p> distribucion.py<pre><code>import pandas as pd\nimport sqlite3\nfrom fitter import Fitter\n\n# Conectar a la base de datos y cargar la tabla en un DataFrame\nconn = sqlite3.connect('proyecto.db')\ndf = pd.read_sql_query(\"SELECT * FROM test_data\", conn)\nconn.close()\n\n# Filtrar los datos para minutes_from_midnight = 626.05\ntarget_minutes = 460.1\nfiltered_data = df[df['minutes_from_midnight'] == target_minutes]['data'].dropna()\n\n# Verificar si hay datos disponibles despu\u00e9s del filtro\nif filtered_data.empty:\n    print(f\"No se encontraron datos para minutes_from_midnight = {target_minutes}.\")\nelse:\n    # Seleccionar las distribuciones que quieres probar\n    distributions = [\n        'norm', 'expon', 'gompertz', 'levy',\n        'logistic', 'rayleigh'\n    ]\n\n    # Ajustar las distribuciones a los datos de 'filtered_data'\n    f = Fitter(filtered_data, distributions=distributions)\n    f.fit()  # Ajusta las distribuciones a los datos\n\n    # Obtener la mejor distribuci\u00f3n para estos datos\n    best_dist = f.get_best()\n    best_dist_name = list(best_dist.keys())[0]  # Nombre de la mejor distribuci\u00f3n\n    best_dist_params = best_dist[best_dist_name]  # Par\u00e1metros de la mejor distribuci\u00f3n\n\n    # Imprimir el resultado\n    print(f\"Para minutes_from_midnight = {target_minutes}, la mejor distribuci\u00f3n es: \"\n          f\"{best_dist_name}, con los par\u00e1metros: {best_dist_params}\")\n</code></pre> <p>Este c\u00f3digo permite obtener el modelo de distribuci\u00f3n que mejor se ajusto con base en los modelos posibles para el proceso. Al realizar diversas pruebas con distintos valores de tiempo, se observa que la distribuci\u00f3n que mejor se ajusta es la exponencial \"expon\", algunos rezultados de las pruebas realizadas a tres instantes distintos se muestran a continuaci\u00f3n:</p> <p><code>Para minutes_from_midnight = -649.85, la mejor distribuci\u00f3n es: expon, con los par\u00e1metros: {'loc': 0.12535911861064752, 'scale': 2.8123496320947368}</code> </p> <p><code>Para minutes_from_midnight = -54.45, la mejor distribuci\u00f3n es: expon, con los par\u00e1metros: {'loc': 0.02121149149570576, 'scale': 0.812807395554624}</code></p> <p><code>Para minutes_from_midnight = 114, la mejor distribuci\u00f3n es: expon, con los par\u00e1metros: {'loc': 0.022537642544328218, 'scale': 0.9522438596968169}</code></p> <p><code>Para minutes_from_midnight = 553.5, la mejor distribuci\u00f3n es: expon, con los par\u00e1metros: {'loc': 0.04768446100892672, 'scale': 2.6305185025999633}</code></p> <p>Como se observa, los par\u00e1metros de la distribuci\u00f3n cambian con el tiempo y siguen la funci\u00f3n de densidad de probabilidad exponencial. Utilizando el programa <code>parameters.py</code> se crea una nueva base de datos llamada <code>parameters.db</code> cuya tabla es <code>parameters_data</code>. Se extraen los datos de timestamp, minutes_from_midnight y sunlight de <code>proyecto.db</code> y luego, con el programa <code>parameters_expon.py</code> se calculan los par\u00e1metros <code>loc</code> y <code>scale</code> de los datos seg\u00fan su timestamp siguiendo una distribuci\u00f3n exponencial aprovechando el comando <code>fit.expon</code>. En la siguiente gr\u00e1fica se muestra el histograma de los datos asociados con la noche (sunlight = 0) con el fin de observar la curva exponencial. </p> <p>Una vez sabiendo el modelo de la distribuci\u00f3n, cuya funci\u00f3n de densidad de probabilidad sigue la siguiente ecuaci\u00f3n:</p> \\[f(x, \\lambda) = \\begin{cases} \\lambda e^{-\\lambda x}, &amp; \\text{si } x \\geq 0 \\\\ 0, &amp; \\text{si } x &lt; 0 \\end{cases}\\] <p>donde \\(\\lambda\\) depende del tiempo, es decir que se puede reescribir de la siguiente forma:</p> \\[f(x, \\lambda(t)) = \\begin{cases} \\lambda(t) e^{-\\lambda(t) x}, &amp; \\text{si } x \\geq 0 \\\\ 0, &amp; \\text{si } x &lt; 0 \\end{cases}\\] <p>Ahora, para encontrar el par\u00e1metro \\(\\lambda\\) se utiliza la media temporal variante, es decir cuando sunlight=1. Para ello, se apela a la siguiente figura:</p> <p></p> <p>Por teor\u00eda, se sabe que la ecuaci\u00f3n de ajuste observada en la gr\u00e1fica es \\(\\mu(t)\\) y que existe una relaci\u00f3n entre el par\u00e1metro \\(\\lambda\\) y \\(\\mu\\) como se muestra en la siguiente ecuaci\u00f3n:</p> \\[ \\mu(t) = \\frac{1}{\\lambda(t)} \\] <p>Para efectos simples, de acuerdo con la documentaci\u00f3n para las funciones de densidad de probabilidad de fitter, el par\u00e1metro loc para una exponencial es t\u00edpicamente cero, mientras que el par\u00e1metro scale tiene la particularidad de ser igual al inverso del par\u00e1metro lambda, es decir que la curva de mejor ajuste para la media variante con el tiempo es igual a la curva de mejor ajuste del par\u00e1metro scale.</p> \\[ \\lambda(t) = \\frac{1}{scale(t)} =\\frac{1}{-0,000017t^{2}+0,024163t-5,505220} \\] <p>Finalmente, la funci\u00f3n de densidad de probabilidad quedar\u00eda de la siguiente manera:</p> \\[f(t, \\lambda(t)) = \\begin{cases} \\left(\\frac{1}{-0,000017t^{2}+0,024163t-5,505220}\\right)e^{-\\left(\\frac{1}{-0,000017t^{2}+0,024163t-5,505220}\\right)t}, &amp; \\text{si } t \\geq 0 \\\\ 0, &amp; \\text{si } t &lt; 0 \\end{cases}\\]"},{"location":"graficas_descriptivas_modelos_probabilidad/","title":"Recopilaci\u00f3n, almacenamiento de datos y gr\u00e1ficas descriptivas","text":"<p>En este apartado se documentar\u00e1 c\u00f3mo se recopilaron los datos y se explorar\u00e1n los programas <code>.py</code>dise\u00f1ados para la creaci\u00f3n de gr\u00e1ficas e histogramas. A continuaci\u00f3n se explicar\u00e1n las librer\u00edas utilizadas para confeccionar la l\u00f3gica de programaci\u00f3n para los programas creados externos a los brindados en el repositorio</p>"},{"location":"graficas_descriptivas_modelos_probabilidad/#1-pandas","title":"1. Pandas","text":"<p>Pandas es una librer\u00eda de python que facilita la manipulaci\u00f3n y el an\u00e1lisis de datos. Proporciona herramientas que facilita el trabajo con tablas de datos.</p> <p>Enfocado a aspectos del proyecto, pandas permite seleccionar y filtrar, a trav\u00e9s de comandos y de forma sencilla los conjuntos de datos que me interesan de una tabla. Por ejemplo los datos de la primera columna o de la segunda seg\u00fan sea el caso.</p>"},{"location":"graficas_descriptivas_modelos_probabilidad/#2-sqlalchemy","title":"2. SQLalchemy","text":"<p>SQLAlchemy es una biblioteca de Python para trabajar con bases de datos relacionales. Es un ORM (Object Relational Mapper) que proporciona una capa de abstracci\u00f3n entre el c\u00f3digo Python y las bases de datos. Algunas caracter\u00edsticas clave de SQLAlchemy son:</p> <ol> <li>Compatibilidad con m\u00faltiples bases de datos (MySQL, PostgreSQL, SQLite, etc.)</li> <li>Mapeo objeto-relacional (ORM) para interactuar con la base de datos usando objetos Python</li> <li>Expresi\u00f3n SQL para construir consultas complejas de manera program\u00e1tica</li> <li>Soporte para transacciones y conexiones pooling</li> <li>Herramientas para migraci\u00f3n de esquemas de base de datos</li> </ol>"},{"location":"graficas_descriptivas_modelos_probabilidad/#3-matplotlib","title":"3. Matplotlib","text":"<p>Matplotlib es una biblioteca de visualizaci\u00f3n de datos en Python, dise\u00f1ada para crear gr\u00e1ficos est\u00e1ticos, animados e interactivos de alta calidad. Es una de las herramientas m\u00e1s populares para la visualizaci\u00f3n de datos en el ecosistema cient\u00edfico de Python. Uno de los aspectos m\u00e1s \u00fatiles es que permite crear una amplia gama de gr\u00e1ficos, incluyendo gr\u00e1ficos de l\u00edneas, de barras, de dispersi\u00f3n, histogramas, gr\u00e1ficos de pastel, gr\u00e1ficos 3D, y muchos m\u00e1s.</p> <p>En resumen, integra aspectos propios de matlab en el ambiente de python para facilitar la creaci\u00f3n de gr\u00e1ficos y el manejo de datos c\u00f3mo se realiza en la plataforma de matlab.</p>"},{"location":"graficas_descriptivas_modelos_probabilidad/#graficas-descriptivas-de-las-variables-1-y-2","title":"Gr\u00e1ficas descriptivas de las variables 1 y 2","text":"<p>A continuaci\u00f3n se muestra el c\u00f3digo empleado para la generaci\u00f3n de los histogramas a partir de los datos guardados en base de datos.</p> <p>graficas_descriptivas<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Cargar la base de datos\ndb_path = 'proyecto.db'  # Aseg\u00farate de que la ruta sea correcta\ndata = pd.read_sql('SELECT variable_1, variable_2 FROM test_data', 'sqlite:///' + db_path)\n\n# Crear figura con tama\u00f1os ajustados\nplt.figure(figsize=(12, 6))\n\n# Crear histograma para variable_1\nplt.subplot(1, 2, 1)\nplt.hist(data['variable_1'], bins=70, color='dodgerblue', edgecolor='black', alpha=0.75)  # Bins para variable 1 en 70\nplt.title('Histograma de Variable 1')\nplt.xlabel('Variable 1')\nplt.ylabel('Frecuencia')\nplt.xlim(0, 4)  # Limitar el eje X para ver mejor la estructura del histograma\nplt.grid(True, axis='y', linestyle='--', alpha=0.6)  # A\u00f1adir una cuadr\u00edcula ligera en el eje Y\n\n# Crear histograma para variable_2 con m\u00e1s bins\nplt.subplot(1, 2, 2)\nplt.hist(data['variable_2'], bins=100, color='darkorange', edgecolor='black', alpha=0.75)  # Aumentar bins a 100 para variable 2\nplt.title('Histograma de Variable 2')\nplt.xlabel('Variable 2')\nplt.ylabel('Frecuencia')\nplt.xlim(0, 12.5)  # Limitar el eje X para ver mejor la estructura del histograma\nplt.grid(True, axis='y', linestyle='--', alpha=0.6)  # A\u00f1adir una cuadr\u00edcula ligera en el eje Y\n\n# Ajustar dise\u00f1o y mostrar gr\u00e1ficas\nplt.tight_layout(pad=3.0)  # Aumentar separaci\u00f3n entre gr\u00e1ficas\nplt.show()\n</code></pre> Se a\u00f1adieron m\u00e1s \"bins\" al programa con el fin de poder visualizar m\u00e1s rect\u00e1ngulos y observar mejor la distribuci\u00f3n de probabilidad. Esto le da mayor resoluci\u00f3n:  </p> <p>Observando las figuras y la curva de mejor ajuste, se puede deducir que, los modelos de distribuci\u00f3n son Rayleigh para la variable 1 y Exponencial para la variable 2. Esto se puede observar de mejor manera en el cuadro a continuaic\u00f3n, que resume los resultados anteriores.</p> <code>variable_1</code> <code>variable_2</code> Modelo Rayleigh Exponencial"},{"location":"momentos_var1_var2/","title":"Momentos de probabilidad de los datos","text":"<p>En esta secci\u00f3n presentamos los momentos estad\u00edsticos calculados para <code>variable_1</code> y <code>variable_2</code> usando dos conjuntos de datos diferentes: un conjunto de datos recolectados durante 12 horas usando los programas <code>tasks.py</code> y <code>models.py</code> y el otro corresponde a una muestra m\u00e1s peque\u00f1a de 50 datos con el fin de aproximar el c\u00e1lculo de los momentos a mano.</p>"},{"location":"momentos_var1_var2/#momentos-de-variable_1-y-variable_2","title":"Momentos de <code>variable_1</code> y <code>variable_2</code>","text":"<p>Para obtener los momentos de las variables se emple\u00f3 el siguiente c\u00f3digo de python que amerit\u00f3 el uso de pandas, sqalchemy y scipy. calculo_momentos.py<pre><code>import pandas as pd\nfrom sqlalchemy import create_engine\nfrom scipy.stats import skew, kurtosis\n\n# Conectar a la base de datos usando SQLAlchemy\nengine = create_engine('sqlite:///proyecto.db')\n\n# Leer los datos de la tabla 'test_data'\ndf = pd.read_sql(\"SELECT variable_1, variable_2 FROM test_data\", engine)\n\n# Calcular momentos\ndef calcular_momentos(serie):\n    momentos = {\n        'promedio': serie.mean(),\n        'varianza': serie.var(),\n        'desviacion_estandar': serie.std(),\n        'inclinacion': skew(serie),\n        'kurtosis': kurtosis(serie)\n    }\n    return momentos\n\n# Aplicar a las dos columnas\nmomentos_variable_1 = calcular_momentos(df['variable_1'])\nmomentos_variable_2 = calcular_momentos(df['variable_2'])\n\n# Mostrar los resultados\n# Funci\u00f3n para imprimir los resultados de manera ordenada\ndef mostrar_momentos(nombre_variable, momentos):\n    print(f\"\\nMomentos de {nombre_variable}:\")\n    print(f\"Promedio:             {momentos['promedio']:.6f}\")\n    print(f\"Varianza:             {momentos['varianza']:.6f}\")\n    print(f\"Desviaci\u00f3n Est\u00e1ndar:  {momentos['desviacion_estandar']:.6f}\")\n    print(f\"Inclinaci\u00f3n (Skew):   {momentos['inclinacion']:.6f}\")\n    print(f\"Kurtosis:             {momentos['kurtosis']:.6f}\")\n\n\n# Mostrar los resultados de manera ordenada\nmostrar_momentos('variable_1', momentos_variable_1)\nmostrar_momentos('variable_2', momentos_variable_2)\n</code></pre></p>"},{"location":"momentos_var1_var2/#conjunto-de-datos-de-12-horas","title":"Conjunto de datos de 12 horas","text":"Momento <code>variable_1</code> <code>variable_2</code> Media 0.806390 1.807410 Varianza 0.157145 1.006807 Desviaci\u00f3n Est\u00e1ndar 0.396416 1.003398 Inclinaci\u00f3n (Skewness) 1.654464 13.782215 Kurtosis 8.756279 768.092391"},{"location":"momentos_var1_var2/#muestra-de-50-datos","title":"Muestra de 50 datos","text":"Momento <code>variable_1</code> <code>variable_2</code> Media 0.845897 1.868556 Varianza 0.156136 0.856203 Desviaci\u00f3n Est\u00e1ndar 0.395141 0.925312 Inclinaci\u00f3n (Skewness) 1.508591 2.791238 Kurtosis 5.875395 12.276760"},{"location":"momentos_var1_var2/#interpretacion-comparativa","title":"Interpretaci\u00f3n Comparativa","text":"<ul> <li> <p>Media: Las medias son relativamente similares entre los dos conjuntos de datos. <code>Variable_2</code> tiene un valor m\u00e1s alto en ambas muestras.</p> </li> <li> <p>Varianza y Desviaci\u00f3n Est\u00e1ndar: <code>Variable_2</code> muestra una mayor variabilidad en ambas muestras, pero en la muestra de 50 datos, los valores son algo menores en comparaci\u00f3n con el conjunto de 12 horas. Esto indica que la dispersi\u00f3n de los datos es m\u00e1s significativa en el conjunto m\u00e1s grande.</p> </li> <li> <p>Inclinaci\u00f3n (Skewness): El sesgo de <code>Variable_2</code> es extremadamente alto en el conjunto de 12 horas (13.78) y m\u00e1s moderado en la muestra de 50 datos (2.79), lo que podr\u00eda indicar la presencia de valores at\u00edpicos en el conjunto mayor. Esto sugiere una distribuci\u00f3n altamente asim\u00e9trica en el conjunto de 12 horas.</p> </li> <li> <p>Kurtosis: La kurtosis de <code>Variable_2</code> en el conjunto de 12 horas es muy alta (768.09), lo que sugiere que la distribuci\u00f3n tiene picos extremadamente pronunciados y una gran cantidad de valores at\u00edpicos. En la muestra de 50 datos, la kurtosis sigue siendo alta (12.28) pero considerablemente menor, lo que indica que la muestra m\u00e1s peque\u00f1a no refleja completamente la presencia de valores extremos.</p> </li> </ul>"},{"location":"parte_2/","title":"Procesos aleatorios","text":""},{"location":"parte_2/#procesos-aleatorios","title":"Procesos aleatorios","text":"<p>En esta segunda parte del proyecto, se buscan estudiar los procesos aleatorios y aspectos de suma importancia como es el caso de la funci\u00f3n de densidad de probabilidad de un proceso aleatorio, definir si un proceso es erg\u00f3digo, si es estacionario en sentido amplio y la funci\u00f3n de densidad de potencia.</p> <p>Durante esta secci\u00f3n se estudiar\u00e1n 3 puntos escenciales:</p> <ol> <li>Determinaci\u00f3n de la funci\u00f3n de densidad de probabilidad</li> <li>Determinaci\u00f3n de la estacionaridad en sentido amplio y ergodicidad</li> <li>Determinaci\u00f3n de potencia promedio</li> </ol> <p>Para cumplir con los puntos anteriormente descritos, se debe primero recopilar durante 24 horas los datos de una API y guardarlos en una base de datos. Luego, se convierte el timestamo de los datos a minutos respecto a la media noche del d\u00eda siguiente. Es decir que, si la toma de datos inici\u00f3 en 2024/11/13 el minuto 0 est\u00e1 dispuesto para 2024/11/14 00:00:00. Entonces, si la toma inici\u00f3 a la 1 de la tarde del 13 de noviembre del 2024, el valor en la columna \"minutes_from_midnight\" para este valor ser\u00e1 -620 minutos. para el caso de la 1 de la tarde del 14 de noviembre, el valor ser\u00e1 de 780 minutos. Esto se logra gracias al c\u00f3digo que se muestra a continuaci\u00f3n:</p> time_since_12.py<pre><code>import pandas as pd\nimport sqlite3\n\n# Conectar a la base de datos y cargar la tabla en un DataFrame\nconn = sqlite3.connect('proyecto.db')\ndf = pd.read_sql_query(\"SELECT * FROM test_data\", conn)\n\n# Convertir 'timestamp' a formato datetime si no lo est\u00e1 ya\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n\n# Definir la medianoche del 14 de noviembre de 2024 (sin necesidad de recalcular en cada fila)\nmidnight_nov_14 = pd.to_datetime('2024-11-14 00:00:00')\n\n# Vectorizaci\u00f3n: Calcular los minutos en funci\u00f3n de si es antes o despu\u00e9s de la medianoche\nbefore_midnight = df['timestamp'] &lt; midnight_nov_14\nafter_midnight = ~before_midnight\n\n# Calcular minutos negativos para los valores antes de la medianoche\ndf.loc[before_midnight, 'minutes_from_midnight'] = (df.loc[before_midnight, 'timestamp'] - midnight_nov_14).dt.total_seconds() / 60\n\n# Calcular minutos positivos para los valores despu\u00e9s de la medianoche\ndf.loc[after_midnight, 'minutes_from_midnight'] = (df.loc[after_midnight, 'timestamp'] - midnight_nov_14).dt.total_seconds() / 60\n\n# Depuraci\u00f3n limitada a las primeras filas\nprint(\"Ejemplo de c\u00e1lculos:\")\nprint(df[['timestamp', 'minutes_from_midnight']].head(10))\n\n# Guardar el DataFrame actualizado en la base de datos\ndf.to_sql('test_data', conn, if_exists='replace', index=False)\nconn.close()\n\nprint(\"Base de datos actualizada con la nueva columna 'minutes_from_midnight'.\")\n</code></pre>"},{"location":"parte_2/#promedio-temporal","title":"Promedio temporal","text":"<p>Gracias a la nueva columna recientemente a\u00f1adida, se pueden separar los datos en bloques de 100 desde el primer bloque y calcular el promedio de cada punto y observarlo en una gr\u00e1fica. Esto se logra gracias al siguiente c\u00f3digo:</p> <p>promedio_temporal.py<pre><code>import pandas as pd\nimport sqlite3\nimport matplotlib.pyplot as plt\n\n# Conectar a la base de datos y cargar la tabla en un DataFrame\nconn = sqlite3.connect('proyecto.db')\ndf = pd.read_sql_query(\"SELECT * FROM test_data\", conn)\n\n# Asegurarse de que la columna 'minutes_from_midnight' est\u00e9 calculada\n# (debe estar calculada previamente como en los pasos anteriores)\n\n# Agrupamos los datos en bloques de 100 elementos\ndf['group'] = df.index // 100  # Crear un grupo por cada 100 datos\n\n# Calcular el promedio de 'data' para cada grupo de 100 datos y asociarlo con 'minutes_from_midnight'\ngrouped_data_avg = df.groupby('group').agg(\n    avg_data=('data', 'mean'),  # Promedio de la columna 'data'\n    minutes_from_midnight=('minutes_from_midnight', 'first'),  # Tomamos el primer valor de 'minutes_from_midnight' para cada grupo\n    sunlight=('sunlight', 'first')  # Tomamos el primer valor de 'sunlight' para cada grupo\n)\n\n# Crear una lista de colores basada en la columna 'sunlight'\ngrouped_data_avg['color'] = grouped_data_avg['sunlight'].apply(lambda x: 'orange' if x else 'blue')\n\n# Graficar el promedio de 'data' a lo largo de 'minutes_from_midnight'\nplt.figure(figsize=(10, 6))\n\n# Graficamos con los colores basados en 'sunlight'\nfor i, row in grouped_data_avg.iterrows():\n    plt.scatter(row['minutes_from_midnight'], row['avg_data'], color=row['color'])\n\n# Etiquetas y t\u00edtulo\nplt.xlabel('Minutos desde la Medianoche')\nplt.ylabel('Promedio de la Columna \"data\"')\nplt.title('Promedio de los Datos a lo Largo del Tiempo con Influencia del \"Sunlight\"')\n\n# Agregar leyenda\nplt.scatter([], [], color='orange', label='Sunlight')\nplt.scatter([], [], color='blue', label='No Sunlight')\nplt.legend()\n\n# Mostrar la gr\u00e1fica\nplt.grid(True)\nplt.show()\n\n# Cerrar la conexi\u00f3n a la base de datos\nconn.close()\n</code></pre> La figura que se observa al ejecutar el c\u00f3digo se muestra a continuaci\u00f3n: </p>"},{"location":"parte_2/#estacionaridad-del-proceso","title":"Estacionaridad del proceso","text":"<p>Observando la figura, se notan dos aspectos interesantes. Los puntos amarillos son aquellos datos que est\u00e1n relacionados cuando es de d\u00eda (sunlight = 1) y los datos azules son aquellos relacionados cuando es de noche (sunlight = 0). Si se toma todo el proceso, es claro que este no es estacionario en sentido amplio. No obstante, cuando es de noche, el promedio permanece relativamente constante, por lo que es necesario realizar pruebas que demuestren si este es estacionario en sentido amplio o no.</p> <p>Condiciones para que un proceso aleatorio sea estacionario en sentido amplio (WSS)</p> <ol> <li>El valor esperado (media) no depende del tiempo: La media del proceso debe ser constante a lo largo del tiempo.</li> <li>La funci\u00f3n de autocovarianza solo depende de la diferencia de tiempos: La autocovarianza debe depender \u00fanicamente de la diferencia entre los tiempos, no de los tiempos absolutos.</li> </ol> <p>Recurriendo nuevamente a la gr\u00e1fica de promedio tempora, se observa que, para sunlight = 0 el promedio temporal puede decirse que permanece constante. Ahora bien, realizando la prueba de autocorrelaci\u00f3n con el programa <code>wss.py</code> se obtienen los resultados que se muestran en la siguiente tabla:</p>"},{"location":"parte_2/#prueba-de-autocorrelacion","title":"Prueba de autocorrelaci\u00f3n","text":"valor 1 valor 2 \\(\\tau\\) \\(R_{xx}(\\tau)\\) -322 -282.75 42.2 0.9999757428 148.2 190.75 42.55 0.9999784399 -63.6 -21.08333332 42.52 0.99998 252.45 294.15 41.7 0.9999755838 <p>Como se observa, al mantener relativamente constante el diferencial de tiempo, la autocorrelaci\u00f3n permanece casi totalmente constante, esto sumado a que la media temporal para sunlight = 0 es tambi\u00e9n relativamente constante alrededor de 1. Es razonable afirmar que el proceso es estacionario en sentido amplio entre las 6 a.m y las 6 p.m</p> <p>Si se realiza este proceso con m\u00e1s instantes de tiempo, se podr\u00e1 observar como la autocorrelaci\u00f3n permanecer\u00e1 constante para todos estos. El proceso no se automatiz\u00f3 debido a las ligeras variaciones en los diferenciales de tiempo. Es decir que la distancia entre cada uno es ligeramente distinta, lo cual hace dificil la creaci\u00f3n de un c\u00f3digo que permita realizar muchas m\u00e1s pruebas.</p>"},{"location":"parte_2/#prueba-de-ergodicidad","title":"Prueba de ergodicidad","text":"<p>Para determinar si un proceso es erg\u00f3dico, es importante resaltar dos aspectos fundamentales:</p> <p>Condiciones para que un proceso aleatorio sea erg\u00f3dico</p> <ol> <li>Estacionaridad en sentido amplio: El proceso debe ser estacionario en sentido amplio (wss).</li> <li>La media temporal igual a la media estad\u00edstica: \\(\\overline{x} = \\overline{X}\\).</li> </ol> <p>Para distinguir la ergodicidad del proceso se emplea el c\u00f3digo de python <code>ergodicidad.py</code> cuyo funcionamiento se basa en tomar el primer valor de cada timestamp de datos cuando sunlight es cero y calcular el promedio de estos. El c\u00f3digo utilizado se muestra a continuaci\u00f3n:</p> <p>ergodicidad.py<pre><code>import sqlite3\n\n# Conectar a la base de datos\ndb_path = \"proyecto.db\"  # Ruta de tu base de datos\nconn = sqlite3.connect(db_path)\ncursor = conn.cursor()\n\n# Consulta SQL para obtener el promedio\nquery = \"\"\"\nWITH RankedData AS (\n    SELECT \n        timestamp, \n        data,\n        ROW_NUMBER() OVER (PARTITION BY timestamp ORDER BY ROWID) AS row_num\n    FROM test_data\n    WHERE sunlight = 0\n)\nSELECT AVG(data) AS average_data\nFROM RankedData\nWHERE row_num = 1;\n\"\"\"\n\n# Ejecutar la consulta\ncursor.execute(query)\nresult = cursor.fetchone()\n\n# Mostrar el resultado\naverage_data = result[0]\nprint(f\"El promedio de los primeros valores de 'data' para cada 'timestamp' con 'sunlight' = 0 es: {average_data}\")\n\n# Cerrar la conexi\u00f3n\nconn.close()\n</code></pre> Al ejecutar el programa se obtiene el siguiente resultado:</p> \\[ \\overline{x} = 1.0228689182508934 \\] <p>Observando nuevamente la gr\u00e1fica de la media temporal \\(\\overline{X}\\) cuando sunlight es cero la media se encuentra cerca de 1, por lo que, dado lo anterior se puede confirmar que hasta cierto punto el proceso es erg\u00f3dico cuando es de noche.</p>"},{"location":"parte_2/#conclusiones-de-seccion","title":"Conclusiones de secci\u00f3n","text":"<p>A partir de las pruebas de estacionaridad y ergodicidad, se pudo observar que, analizando la totalidad del proceso este no se considerar\u00eda estacionario en sentido \u00e1mplio (wss) pero, cortando una secci\u00f3n de tiempo donde el proceso parece ser constante y realizando las pruebas de estacionaridad, se deduce que, para la noche el proceso es estacionario en sentido amplio. Por otra parte, a partir de las pruebas de ergodicidad se concluye que la media estad\u00edstica es igual a la media temporal, por lo tanto, el proceso es erg\u00f3dico durante la noche</p>"},{"location":"potencia_promedio/","title":"Potencia promedio del proceso aleatorio","text":""},{"location":"potencia_promedio/#potencia-promedio-de-un-proceso-aleatorio","title":"Potencia promedio de un proceso aleatorio","text":"<p>La potencia de un proceso aleatorio puede definirse como el valor esperado del valor cuadr\u00e1tico medio como se observa en la siguiente ecuaci\u00f3n:</p> \\[ P = E[\\overline{X}(t)^{2}] \\] <p>Esto puede ser aplicado directamente cuando la potencia no var\u00eda con el tiempo. Es decir, cuando el proceso es estacionario en sentido amplio. Cuando esta var\u00eda respecto al tiempo como es el caso de los datos cuando sunlight es 1 se debe aplicar la siguiente f\u00f3rmula a trav\u00e9s del uso de <code>scypy integrate</code></p> \\[ \\frac{1}{t_2 - t_1} {\\int_{t_1}^{t_2}{X^{2}(t)dt}} \\] <p>Esta ecuaci\u00f3n lo que obtiene es el promedio temporal del valor cuadr\u00e1tico medio, es lo mismo que para el est\u00e1tico pero este a\u00f1ade el factor de la varianza temporal.</p> <p>A continuaci\u00f3n se observa la gr\u00e1fica del promedio del cuadrado de los datos a lo largo del tiempo de muestreo:</p> <p></p> <p>Como se observa, la gr\u00e1fica sigue la misma tendencia que el promedio de los datos a lo largo del tiempo. La potencia promedio se dividir\u00e1 en dos, la est\u00e1tica para cuando sunlight es cero y la din\u00e1mica, cuando sunlight es 1.</p>"},{"location":"potencia_promedio/#potencia-promedio-estatica","title":"Potencia promedio est\u00e1tica","text":"<p>Cuando sunlight es cero, los el promedio del cuadrado de los datos tienden a mantenerse cercanos a un valor, en este caso se us\u00f3 el programa <code>potencia_promedio_temporal.py</code> para calcular el promedio del cuadrado de los valores originales cuando sunlight es cero, filtrando los resultados para cuando es uno. de forma que se obtiene la siguiente figura:</p> <p></p> <p>Como se observa, el valor esperado de los datos cuadr\u00e1ticos cuando es de noche es 2.00, este valor fue sencillo de conseguir debido a las caracter\u00edsticas temporales que posee el proceso para estas condiciones</p>"},{"location":"potencia_promedio/#potencia-promedio-dinamica","title":"Potencia promedio din\u00e1mica","text":"<p>A continuaci\u00f3n, a partir del programa <code>potencia_promedio_dinamica.py</code> se logran filtrar los valores para cuando es de noche, dejando \u00fanicamente la parte del proceso que var\u00eda con el tiempo. Luego, a la etiqueta temporal de los valores negativos le suma 1400 para construir de mejor forma la curva de segundo orden. Por \u00faltimo, se obtiene la curva de mejor ajuste de esta gr\u00e1fica, la cual sirvi\u00f3 para obtener el promedio a trav\u00e9s de la integral utilizando <code>scipy integrate</code></p> <p></p> <p>Ahora, el programa aplica el valor esperado utilizando esta curva de mejor ajuste como funci\u00f3n \\(X(t)\\) de la siguiendo la siguiente ecuaci\u00f3n:</p> \\[ \\frac{1}{t_2 - t_1} {\\int_{t_1}^{t_2}{(-0.0001t^{2}+0.2025t-54.1792)dt}} \\] <p>El programa mismo calcula los valores de \\(t_2\\) y \\(t_1\\) como los valores de tiempo mayor y menor de todos los datos de \"minutes_from_midnight\" recopilados. </p> <p>Ahora bien, el programa indic\u00f3 que el valor de potencia promedio temporal obtenida es de 11.1619. En la siguiente tabla se resumen los resultados obtenidos:</p> Potencia promedio para sunlight = 0 Potencia promedio para sunlight = 1 2.00 11.1619"}]}